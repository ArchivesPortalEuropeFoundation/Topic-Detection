{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import nltk, string, pickle\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# this function convert a string of text to a cross-lingual doc embedding capturing its semantics\n",
    "def text_embedding(text,model):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "        \n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    doc_embedd = []\n",
    "    \n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = model[word]\n",
    "                doc_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    if len(doc_embedd)>1:\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*doc_embedd)]\n",
    "        return avg\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the dataset\n",
    "\n",
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each language under study you need to download its related cross-lingual embeddings from here: https://github.com/facebookresearch/MUSEÅ“\n",
    "\n",
    "de_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.de.vec')\n",
    "fr_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.fr.vec')\n",
    "en_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.en.vec')\n",
    "it_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.it.vec')\n",
    "fi_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.fi.vec')\n",
    "pl_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.pl.vec')\n",
    "sl_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.sl.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just map the language with the word embeddings model\n",
    "\n",
    "model_dict = {\"fr\":fr_model,\"en\":en_model,\"de\":de_model,\"it\":it_model,\"fi\":fi_model,\"pl\":pl_model,\"sl\":sl_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we want to skip a specific topic\n",
    "skip = [\"\"]\n",
    "#skip = [\"germanDemocraticRepublic.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# a quick overview of the frequency of language\n",
    "\n",
    "langs = []\n",
    "check = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"langMaterial\"] not in model_dict:\n",
    "        check.append(row[\"langMaterial\"])\n",
    "    else:\n",
    "        langs.append(row[\"langMaterial\"])\n",
    "        \n",
    "\n",
    "print(Counter(langs).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document we create a document embedding and collect its topic label\n",
    "\n",
    "embs = []\n",
    "labels = []\n",
    "selected_langs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    lang = row[\"langMaterial\"]\n",
    "    label = row[\"filename\"]\n",
    "    if lang in model_dict and label not in skip:\n",
    "        model = model_dict[lang]\n",
    "        text = row[\"unitTitle\"] +\" \"+ row[\"titleProper\"]+\" \"+ row[\"scopeContent\"]\n",
    "        emb = text_embedding(text,model)\n",
    "        if emb != None:\n",
    "            embs.append(emb)\n",
    "            labels.append(label)\n",
    "            selected_langs.append(lang)\n",
    "print (len(embs),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview of relation between label and language\n",
    "\n",
    "check = []\n",
    "for x in range(len(labels)):\n",
    "    lang = selected_langs[x]\n",
    "    label = labels[x]\n",
    "    check.append(label+\" \"+lang)\n",
    "Counter(check).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(embs)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we evaluate the quality of the classifier via 10fold cross validation\n",
    "# we report for each fold precision recall f1 and micro f1\n",
    "\n",
    "SVM = svm.SVC(kernel = \"linear\", C=1,probability=True)\n",
    "\n",
    "kf = KFold(n_splits=10,shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    classifier = SVM.fit(X_train , y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    p,r,f1,s = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "    micro_f1 = precision_recall_fscore_support(y_test, y_pred, average=\"micro\")[0]\n",
    "    print (\"p\",round(p,2),\"r\",round(r,2),\"f1\",round(f1,2),\"micro_f1\",round(micro_f1,2))\n",
    "    print (Counter(y_test).most_common())\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train a final model\n",
    "\n",
    "classifier = SVM.fit(X , y)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'trained_topic_classifier.model'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pretrained model\n",
    "\n",
    "with open('trained_topic_classifier.model', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "# get its label\n",
    "cl_labels = classifier.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the classifier to a new description\n",
    "\n",
    "description = \"this is a text about the GDR and Berlin\"\n",
    "lang = \"en\"\n",
    "\n",
    "model = model_dict[lang]\n",
    "emb = text_embedding(description,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = classifier.predict_proba([emb])[0]\n",
    "preds = [[cl_labels[x],pred_proba[x]] for x in range(len(pred_proba))]\n",
    "preds.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# the probability of each topic\n",
    "for x in preds:\n",
    "    print (x[0],round(x[1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic)",
   "language": "python",
   "name": "topic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
