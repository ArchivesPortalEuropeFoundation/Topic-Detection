{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this, only useful when testing it on Colab\n",
    "\n",
    "#!wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.de.vec\n",
    "#!wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec\n",
    "#!wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec\n",
    "#!wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.it.vec\n",
    "#!wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fi.vec\n",
    "#!wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.pl.vec\n",
    "#!wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.sl.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import nltk, string, pickle\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "exclude.add(\"-\")\n",
    "\n",
    "# this function convert a string of text to a cross-lingual doc embedding capturing its semantics\n",
    "def text_embedding(text,model):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "        \n",
    "    text = [token for token in text if token.isalpha()]\n",
    "    \n",
    "    doc_embedd = []\n",
    "    \n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = model[word]\n",
    "                doc_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    if len(doc_embedd)>0:\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*doc_embedd)]\n",
    "        return avg\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the dataset\n",
    "\n",
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each language under study you need to download its related cross-lingual embeddings from here: https://github.com/facebookresearch/MUSEÅ“\n",
    "\n",
    "de_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.de.vec')\n",
    "fr_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.fr.vec')\n",
    "en_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.en.vec')\n",
    "it_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.it.vec')\n",
    "fi_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.fi.vec')\n",
    "pl_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.pl.vec')\n",
    "sl_model = KeyedVectors.load_word2vec_format('word-embs/wiki.multi.sl.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just map the language with the word embeddings model\n",
    "\n",
    "model_dict = {\"fr\":fr_model,\"en\":en_model,\"de\":de_model,\"it\":it_model,\"fi\":fi_model,\"pl\":pl_model,\"sl\":sl_model,\"German\":de_model,\"English\":en_model,\"Finnish\":fi_model,\"French\":fr_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document we create a document embedding and collect its topic label\n",
    "\n",
    "embs = []\n",
    "labels = []\n",
    "doc_names = []\n",
    "selected_langs = []\n",
    "texts = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    lang = row[\"langMaterial\"]\n",
    "    label = row[\"filename\"].replace(\".json\",\"\")    \n",
    "    title = row[\"titleProper\"]\n",
    "\n",
    "    if lang in model_dict:\n",
    "        model = model_dict[lang]\n",
    "        text = row[\"unitTitle\"] +\" \"+ row[\"titleProper\"]+\" \"+ row[\"scopeContent\"]\n",
    "        emb = text_embedding(text,model)\n",
    "        if emb != None:\n",
    "            embs.append(emb)\n",
    "            labels.append(label)\n",
    "            selected_langs.append(lang)\n",
    "            doc_names.append(title)\n",
    "            texts.append(text)\n",
    "print (len(embs),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(v1,v2):\n",
    "    v1 = np.array(v1).reshape(1, -1)\n",
    "    v2 = np.array(v2).reshape(1, -1)\n",
    "    score = cs(v1,v2)[0][0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the classifier to a new description\n",
    "\n",
    "description = \"this is a text about the GDR and Berlin\"\n",
    "lang = \"en\"\n",
    "how_many_results = 50\n",
    "\n",
    "model = model_dict[lang]\n",
    "emb = text_embedding(description,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ranking = [[doc_names[x],labels[x],texts[x],embs[x],cossim(embs[x],emb)] for x in range(len(embs))]\n",
    "\n",
    "ranking.sort(key=lambda x: x[4],reverse=True)\n",
    "\n",
    "ranking = ranking[:how_many_results]\n",
    " \n",
    "rel_topics = set([x[1] for x in ranking])\n",
    "\n",
    "all_res_embs = [float(sum(col))/len(col) for col in zip(*[x[3] for x in ranking])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,pandas\n",
    "\n",
    "all_topic_words = []\n",
    "\n",
    "for topic in rel_topics:\n",
    "    for file in os.listdir(\"taxonomies/\"):\n",
    "        if topic.lower() in file.lower():\n",
    "            file_errors_location = 'taxonomies/'+file\n",
    "            df = pd.read_excel(file_errors_location)\n",
    "            words = df.iloc[:,0].tolist()\n",
    "            words = [x.split(\"/\")[-1].replace(\"_\",\" \") for x in words]\n",
    "            langs = df.iloc[:,1].tolist()\n",
    "            emb = text_embedding(description,model_dict[lang])\n",
    "            topic_words = [[words[x],langs[x],text_embedding(words[x],model_dict[langs[x]])] for x in range(len(words)) if langs[x] in model_dict]\n",
    "            miss_emb = len(topic_words)\n",
    "            topic_words = [x for x in topic_words if x[2] is not None]\n",
    "            miss_emb -= len(topic_words)\n",
    "            print (\"Missing Langs:\",set([langs[x] for x in range(len(langs)) if langs[x] not in model_dict]), \"Missing Embs:\",miss_emb)\n",
    "            all_topic_words += topic_words\n",
    "\n",
    "            \n",
    "topic_ranking = [[topic_words[x][0],cossim(topic_words[x][2],all_res_embs)] for x in range(len(topic_words))]\n",
    "\n",
    "topic_ranking.sort(key=lambda x: x[1],reverse=True)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Most relevant topics:\", \", \".join([x[0] for x in topic_ranking[:10]]))\n",
    "print (\" \")\n",
    "for doc in ranking:\n",
    "    print (doc[0:2],round(doc[-1],3))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
